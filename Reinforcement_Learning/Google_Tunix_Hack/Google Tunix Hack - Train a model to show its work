{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceType":"competition","sourceId":119261,"databundleVersionId":14363498},{"sourceType":"modelInstanceVersion","sourceId":85992,"databundleVersionId":9247169,"modelInstanceId":72251}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Purpose of the Notebook\n\nThis notebook fine-tunes a small **Gemma model** using **reinforcement learning** so that it can:\n\n- üß† **Generate step-by-step reasoning**\n- üî¢ **Produce a final numeric answer**\n- üè∑Ô∏è **Follow a strict format** using XML-like tags (`<reasoning>...</reasoning>` and `<answer>...</answer>`)\n- üìà **Improve reasoning performance** based on custom reward functions\n\nThe core training method used is **GRPO (Group Relative Policy Optimization)** ‚Äî a lightweight, memory-efficient PPO-style RL algorithm implemented in **Tunix**.\n","metadata":{}},{"cell_type":"markdown","source":"# Training Gemma to Think ‚Äì Notebook Walkthrough\n\n## 1. Notebook Goal\n\n- Fine-tune a small Gemma model with reinforcement learning so that it:\n\n    - Writes step-by-step reasoning.\n\n    - Returns a final numeric answer.\n\n    - Follows a strict format with:\n\n        - <reasoning> ... </reasoning>\n\n        - <answer> ... </answer>.\n\n- Use GRPO (Group Relative Policy Optimization), a lightweight PPO-style RL algorithm from Tunix, to improve reasoning ability on GSM8K math problems.\n\n- Run efficiently on a single TPU v5e-8 in Kaggle.","metadata":{}},{"cell_type":"markdown","source":"# 2. Environment Setup ‚Äì CHUNK 0\n\n- Installs all external libraries needed in the notebook:\n\n    - kagglehub to download models/datasets from Kaggle Models & Datasets.\n\n    - tensorflow, tensorflow_datasets, tensorboardX for data + logging.\n\n    - transformers for tokenization utilities.\n\n    - grain for streaming datasets compatible with Tunix.\n\n    - google-tunix[prod]==0.1.3 for RL, GRPO, and Gemma integration.\n\n    - flax (latest) for NNX model definitions.\n\n    - datasets and wandb for data loading and optional tracking.\n\n- This cell is run once, then you Restart the kernel so that JAX/TensorFlow see the newly installed packages.","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CHUNK 0 ‚Äì installs (run once, then Restart)\n# ============================================\n!pip install -q kagglehub ipywidgets tensorflow tensorflow_datasets tensorboardX\n!pip install -q transformers grain \"google-tunix[prod]==0.1.3\"\n!pip uninstall -q -y flax\n!pip install -q -U flax\n!pip install -q datasets wandb\n","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-11-23T20:46:49.925262Z","shell.execute_reply.started":"2025-11-23T20:46:36.012188Z","shell.execute_reply":"2025-11-23T20:46:49.924077Z"}},"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# 3. Imports, TPU Check & Global Config ‚Äì CHUNK 1\n\n- Imports core libraries:\n\n    - Standard Python (os, functools, gc, re, shutil, Path).\n\n    - Flax / JAX / Optax for model, training, and optimization.\n\n    - Tunix modules:\n\n        - sampler_lib for text generation.\n\n        - tokenizer_adapter for tokenizer.\n\n        - model/params for Gemma3.\n\n        - RL utilities: rl_cluster_lib, GRPOConfig, GRPOLearner, base_rollout.\n\n        - metrics_logger for training logs.\n\n    - grain, humanize, tensorflow_datasets, datasets for data.\n\n- Defines hyper-parameters roughly matching the official GRPO Gemma3 tutorial:\n\n    - Data paths, LoRA rank/alpha, mesh layout, sequence lengths.\n\n    - Training batch sizes, number of batches, epochs, and MAX_STEPS.\n\n    - AdamW optimizer settings, warmup schedule, gradient clipping, checkpoint dirs.\n\n- Checks that TPU v5e-8 is visible:\n\n    - Prints JAX devices and warns if no TPU is detected.\n\n- Includes a small utility function show_hbm_usage() to monitor TPU memory.","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CHUNK 1 ‚Äì imports & basic setup\n# ============================================\nimport os, gc, csv, shutil, functools, re\nfrom pprint import pprint\nfrom pathlib import Path\n\nimport grain\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport optax\nimport grain\nimport humanize\nimport kagglehub\nimport tensorflow_datasets as tfds\nfrom datasets import load_dataset\nfrom orbax import checkpoint as ocp\n\nfrom tunix.generate import sampler as sampler_lib\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\nfrom tunix.models.gemma3 import params, model\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.sft import metrics_logger\n\n# TPU / JAX info\nprint(jax.devices())\n\n# =========================\n# Hyperparameters\n# =========================\nTRAIN_DATA_DIR = \"./data/train\"\nTEST_DATA_DIR = \"./data/test\"\nTRAIN_FRACTION = 1.0\n\nRANK = 32\nALPHA = 32.0\n\nMESH = [(1, 4), (\"fsdp\", \"tp\")]\n\nMAX_PROMPT_LENGTH = 256\nTOTAL_GENERATION_STEPS = 512\nTEMPERATURE = 0.9\nTOP_P = 1.0\nTOP_K = 50\nNUM_GENERATIONS = 4\nNUM_ITERATIONS = 1\nBETA = 0.08\nEPSILON = 0.2\n\nTRAIN_MICRO_BATCH_SIZE = 2\nNUM_BATCHES = 3738\nNUM_TEST_BATCHES = 100\nNUM_EPOCHS = 1\nEVAL_EVERY_N_STEPS = 10\n\nMAX_STEPS = int(NUM_BATCHES * NUM_ITERATIONS * TRAIN_FRACTION * NUM_EPOCHS)\nWARMUP_STEPS = int(0.1 * MAX_STEPS)\n\nLEARNING_RATE = 3e-6\nB1 = 0.9\nB2 = 0.99\nWEIGHT_DECAY = 0.1\nMAX_GRAD_NORM = 0.1\n\nINTERMEDIATE_CKPT_DIR = \"/tmp/content/intermediate_ckpt/\"\nCKPT_DIR = \"/tmp/content/ckpts/\"\nSAVE_INTERVAL_STEPS = 500\nMAX_TO_KEEP = 4\n\nGENERATION_CONFIGS = {\n    \"greedy\":   {\"temperature\": 1e-4, \"top_k\": 1,   \"top_p\": 1.0},\n    \"standard\": {\"temperature\": 0.7,  \"top_k\": 50,  \"top_p\": 0.95},\n    \"liberal\":  {\"temperature\": 0.85, \"top_k\": 2000,\"top_p\": 1.0},\n}\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:46:49.925756Z","iopub.execute_input":"2025-11-23T20:46:49.925942Z","iopub.status.idle":"2025-11-23T20:47:07.064016Z","shell.execute_reply.started":"2025-11-23T20:46:49.925923Z","shell.execute_reply":"2025-11-23T20:47:07.062947Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.12/site-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n  warnings.warn(\nWARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1763930816.839525    1696 common_lib.cc:650] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:238\n","output_type":"stream"},{"name":"stdout","text":"[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=4, process_index=0, coords=(0,2,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(1,2,0), core_on_chip=0), TpuDevice(id=6, process_index=0, coords=(0,3,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,3,0), core_on_chip=0)]\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# 4. Model & Tokenizer Loading ‚Äì CHUNK 2\n\n- Handles Kaggle authentication:\n\n    - Tries to read API credentials from UserSecretsClient.\n\n    - Falls back to kagglehub.login() if not present.\n\n- Clears any old checkpoints in /tmp/content/intermediate_ckpt and /tmp/content/ckpts to avoid mixing runs.\n\n- Selects Gemma3-1B-IT as the base model:\n\n    - Uses Tunix params helpers to:\n\n        - Load official Gemma3 weights (MODEL_CP_PATH).\n\n        - Build a ModelConfig (gemma3_1b()).\n\n        - Create a Flax NNX model and tokenizer.\n\n- Saves an intermediate NNX-friendly checkpoint:\n\n    - Splits model into graph and state via nnx.split.\n\n    - Uses orbax StandardCheckpointer to save state to INTERMEDIATE_CKPT_DIR/state.\n\n    - Frees the temporary model (del + gc.collect()) to save memory.\n\n- Defines helper functions:\n\n    - get_gemma_ref_model(ckpt_path):\n\n        - Creates a JAX mesh using MESH.\n\n        - Builds an ‚Äúabstract‚Äù Gemma3 model to infer shapes.\n\n        - Restores parameters from the checkpoint into a sharded NNX model.\n\n        - Returns (gemma_model, mesh, model_config).\n\n    - get_lora_model(base_model, mesh):\n\n        - Uses Qwix LoraProvider to apply LoRA to attention + MLP layers.\n\n        - Shards LoRA-augmented parameters across the mesh.\n\n        - Returns a trainable LoRA policy model.\n\n- Finally:\n\n    - Loads reference model ref_model from the intermediate checkpoint.\n\n    - Creates the LoRA policy model lora_policy that will be updated during GRPO training.","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CHUNK 2 ‚Äì model + tokenizer + LoRA loading\n# ============================================\nfrom kaggle_secrets import UserSecretsClient\n\nimport os\nfrom kaggle_secrets import UserSecretsClient\n\ndef auto_login():\n    client = UserSecretsClient()\n    token = client.get_secret(\"KAGGLE_API_TOKEN\")\n    if token:\n        os.environ[\"KAGGLE_API_TOKEN\"] = token\n        print(\"‚úÖ Loaded KAGGLE_API_TOKEN from Secrets.\")\n    else:\n        print(\"‚ö†Ô∏è No KAGGLE_API_TOKEN secret found, falling back to interactive login.\")\n        import kagglehub\n        kagglehub.login()\n\nauto_login()\n\n\n# Clean old ckpts\n!rm /tmp/content/intermediate_ckpt/* -rf\n!rm /tmp/content/ckpts/* -rf\n\nmodel_family = \"gemma3\"\nif model_family == \"gemma3\":\n    MODEL_CP_PATH = params.GEMMA3_1B_IT\n    config = model.ModelConfig.gemma3_1b()\n    gemma = params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n    tokenizer = params.create_tokenizer()\n\n    checkpointer = ocp.StandardCheckpointer()\n    _, state = nnx.split(gemma)\n    checkpointer.save(os.path.join(INTERMEDIATE_CKPT_DIR, \"state\"), state)\n    checkpointer.wait_until_finished()\n    del params, gemma, state\n    gc.collect()\n\nfrom tunix.models.gemma3 import params  # re-import for get_gemma_ref_model\n\ndef get_gemma_ref_model(ckpt_path):\n    mesh = jax.make_mesh(*MESH)\n    model_config = model.ModelConfig.gemma3_1b()\n    abs_gemma: nnx.Module = nnx.eval_shape(\n        lambda: params.create_model_from_checkpoint(MODEL_CP_PATH, config)\n    )\n\n    abs_state = nnx.state(abs_gemma)\n    abs_state = jax.tree.map(\n        lambda a, s: jax.ShapeDtypeStruct(a.shape, jnp.bfloat16, sharding=s),\n        abs_state,\n        nnx.get_named_sharding(abs_state, mesh),\n    )\n    checkpointer = ocp.StandardCheckpointer()\n    restored_params = checkpointer.restore(ckpt_path, target=abs_state)\n\n    graph_def, _ = nnx.split(abs_gemma)\n    gemma_model = nnx.merge(graph_def, restored_params)\n    return gemma_model, mesh, model_config\n\nimport qwix\n\ndef get_lora_model(base_model, mesh):\n    lora_provider = qwix.LoraProvider(\n        module_path=(\n            \".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj|\"\n            \".*attn_vec_einsum\"\n        ),\n        rank=RANK,\n        alpha=ALPHA,\n    )\n    model_input = base_model.get_model_input()\n    lora_model = qwix.apply_lora_to_model(\n        base_model, lora_provider, **model_input\n    )\n    with mesh:\n        state = nnx.state(lora_model)\n        pspecs = nnx.get_partition_spec(state)\n        sharded_state = jax.lax.with_sharding_constraint(state, pspecs)\n        nnx.update(lora_model, sharded_state)\n    return lora_model\n\n# Load reference and policy models\nref_model, mesh, model_config = get_gemma_ref_model(\n    ckpt_path=os.path.join(INTERMEDIATE_CKPT_DIR, \"state\")\n)\nlora_policy = get_lora_model(ref_model, mesh=mesh)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:50:59.174793Z","iopub.execute_input":"2025-11-23T20:50:59.175201Z","iopub.status.idle":"2025-11-23T20:51:33.924522Z","shell.execute_reply.started":"2025-11-23T20:50:59.175180Z","shell.execute_reply":"2025-11-23T20:51:33.923049Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded KAGGLE_API_TOKEN from Secrets.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\nWARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n/tmp/ipykernel_1696/3686306040.py:44: DeprecationWarning: The default axis_types will change in JAX v0.9.0 to jax.sharding.AxisType.Explicit. To maintain the old behavior, pass `axis_types=(jax.sharding.AxisType.Auto,) * len(axis_names)`. To opt-into the new behavior, pass `axis_types=(jax.sharding.AxisType.Explicit,) * len(axis_names)\n  mesh = jax.make_mesh(*MESH)\nWARNING:absl:`StandardCheckpointHandler` expects a target tree to be provided for restore. Not doing so is generally UNSAFE unless you know the present topology to be the same one as the checkpoint was saved under.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"5. Data & Prompt Template (GSM8K) ‚Äì CHUNK 3\n\nDefines special tokens:\n\nreasoning_start = \"<reasoning>\", reasoning_end = \"</reasoning>\".\n\nsolution_start = \"<answer>\", solution_end = \"</answer>\".\n\nCreates a system prompt and a chat-style template (TEMPLATE) that:\n\nWraps the instruction and question inside <start_of_turn>user / <start_of_turn>model.\n\nTells the model explicitly to:\n\nThink inside <reasoning> ‚Ä¶ </reasoning>.\n\nPut the final numeric answer inside <answer> ‚Ä¶ </answer>.\n\nImplements dataset loading with get_dataset:\n\nSupports multiple sources:\n\n\"tfds\": downloads GSM8K from tensorflow_datasets.\n\n\"kaggle\": downloads a CSV GSM8K version via kagglehub.\n\n\"huggingface\": loads GSM8K via datasets.load_dataset(\"gsm8k\", \"main\").\n\nNormalises every example into a dict with:\n\n\"prompts\": the full input string passed to the model (system prompt + question).\n\n\"question\": raw question text.\n\n\"answer\": numeric answer extracted from the GSM8K #### format.\n\nWraps the data in a grain MapDataset:\n\nShuffles with a fixed seed.\n\nApplies a .map() step to build the prompt/answer format used during training and reward computation.\n\nSplits into:\n\ntrain_dataset: repeated over NUM_EPOCHS.\n\nOptional val_dataset (if TRAIN_FRACTION < 1).\n\ntest_dataset: a held-out subset for evaluation.\n\nPrints dataset lengths and shows a sample batch for sanity checking.","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CHUNK 3 ‚Äì GSM8K data loading (as in tutorial)\n# ============================================\nreasoning_start = \"<reasoning>\"\nreasoning_end = \"</reasoning>\"\nsolution_start = \"<answer>\"\nsolution_end = \"</answer>\"\n\nSYSTEM_PROMPT = f\"\"\"You are given a problem. Think about the problem and \\\nprovide your reasoning. Place it between {reasoning_start} and \\\n{reasoning_end}. Then, provide the final answer (i.e., just one numerical \\\nvalue) between {solution_start} and {solution_end}.\"\"\"\n\nTEMPLATE = \"\"\"<start_of_turn>user\n{system_prompt}\n\n{question}<end_of_turn>\n<start_of_turn>model\"\"\"\n\ndef extract_hash_answer(text: str) -> str | None:\n    if \"####\" not in text:\n        return None\n    return text.split(\"####\")[1].strip()\n\ndef download_kaggle_dataset(target_dir=\"./data/gsm8k\"):\n    os.makedirs(target_dir, exist_ok=True)\n    src = kagglehub.dataset_download(\"thedevastator/grade-school-math-8k-q-a\")\n    src = Path(src)\n    dst = Path(target_dir)\n    for csv_file in src.glob(\"*.csv\"):\n        shutil.copy2(csv_file, dst / csv_file.name)\n        print(f\"Copied {csv_file.name} ‚Üí {dst/csv_file.name}\")\n    return target_dir\n\ndef get_dataset(data_dir, split=\"train\", source=\"huggingface\") -> grain.MapDataset:\n    if not os.path.exists(data_dir):\n        os.makedirs(data_dir)\n\n    if source == \"tfds\":\n        import tensorflow_datasets.text.gsm8k\n        data = tfds.data_source(\n            \"gsm8k\",\n            split=split,\n            data_dir=data_dir,\n            builder_kwargs={\"file_format\": tfds.core.FileFormat.ARRAY_RECORD},\n            download=True,\n        )\n    elif source == \"kaggle\":\n        kaggle_dir = download_kaggle_dataset(data_dir)\n        file_name = \"main_\" + split + \".csv\"\n        csv_path = os.path.join(kaggle_dir, file_name)\n        data = []\n        with open(csv_path, newline=\"\", encoding=\"utf-8\") as csvfile:\n            reader = csv.DictReader(csvfile)\n            for row in reader:\n                data.append({\"question\": row[\"question\"], \"answer\": row[\"answer\"]})\n    elif source == \"huggingface\":\n        os.environ[\"HF_HUB_DISABLE_XET\"] = \"1\"\n        data = load_dataset(\"gsm8k\", \"main\", split=split)\n    else:\n        raise ValueError(f\"Unknown source: {source}\")\n\n    def _as_text(v):\n        return v if isinstance(v, str) else v.decode(\"utf-8\")\n\n    dataset = (\n        grain.MapDataset.source(data)\n        .shuffle(seed=42)\n        .map(\n            lambda x: {\n                \"prompts\": TEMPLATE.format(\n                    system_prompt=SYSTEM_PROMPT,\n                    question=_as_text(x[\"question\"]),\n                ),\n                \"question\": _as_text(x[\"question\"]),\n                \"answer\": extract_hash_answer(_as_text(x[\"answer\"])),\n            }\n        )\n    )\n    return dataset\n\nsource = \"huggingface\"\nprint(f\"Using data source: {source}\")\n\ndataset = get_dataset(TRAIN_DATA_DIR, \"train\", source).batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_BATCHES]\n\nif TRAIN_FRACTION == 1.0:\n    train_dataset = dataset.repeat(NUM_EPOCHS)\n    val_dataset = None\nelse:\n    train_dataset = dataset[: int(len(dataset) * TRAIN_FRACTION)].repeat(NUM_EPOCHS)\n    val_dataset = dataset[int(len(dataset) * TRAIN_FRACTION) :].repeat(NUM_EPOCHS)\n\ntest_dataset = get_dataset(TEST_DATA_DIR, \"test\", source).batch(TRAIN_MICRO_BATCH_SIZE)[:NUM_TEST_BATCHES]\n\nprint(\"dataset lengths (train, val, test):\",\n      len(train_dataset),\n      len(val_dataset) if val_dataset is not None else 0,\n      len(test_dataset))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:51:58.569345Z","iopub.execute_input":"2025-11-23T20:51:58.569647Z","iopub.status.idle":"2025-11-23T20:52:01.866459Z","shell.execute_reply.started":"2025-11-23T20:51:58.569625Z","shell.execute_reply":"2025-11-23T20:52:01.865250Z"}},"outputs":[{"name":"stdout","text":"Using data source: huggingface\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6acd4bc5786f4300b7174d9fc6579e27"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"main/train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3088a78d451f464b962fe54aec6f3397"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"main/test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3f4c603031448585bad1aa00c9ad50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fd05bc66761244f39c24c5093850f266"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f1fe20e6df54a4a8e9793d9c4eda82b"}},"metadata":{}},{"name":"stdout","text":"dataset lengths (train, val, test): 3737 0 100\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"6. Reward Functions for GRPO ‚Äì CHUNK 4\n\nDefines a regex match_format to check strict format:\n\nEnsures the response contains:\n\nExactly one <reasoning> ... </reasoning> block.\n\nExactly one <answer> ... </answer> block.\n\nAllows spaces/newlines but nothing outside those tags.\n\nReward functions:\n\nmatch_format_exactly\n\nReturns 3.0 if the output perfectly matches the required format.\n\nReturns 0 otherwise.\n\nEncourages the model to always use the XML-like reasoning + answer blocks.\n\nmatch_format_approximately\n\nGives small positive or negative scores based on:\n\nHow many times each of the four tags appears.\n\nReward if each tag appears exactly once, penalise if missing or repeated.\n\nHelps nudging the model toward proper structure even before it‚Äôs perfect.\n\ncheck_answer\n\nExtracts the plain answer from <answer> ... </answer>.\n\nCompares with the ground-truth numeric answer.\n\nRewards:\n\n+3.0 for an exact match.\n\n+1.5 if equal up to whitespace.\n\nSmaller positive scores if the ratio guess / true_answer is close to 1.\n\nNegative penalty when the answer is clearly wrong.\n\nDrives the model to get numerically correct answers.\n\ncheck_numbers\n\nUses another regex to extract the first number inside the <answer> block.\n\nCompares that number to the true answer.\n\nGives +1.5 for exact numeric match, 0 otherwise.\n\nActs as a backup when the answer is embedded in a larger sentence.\n\nTogether, these reward functions push the model to:\n\nUse the desired format.\n\nGive correct (or near-correct) numeric answers.\n\nAvoid random or badly formatted outputs.","metadata":{}},{"cell_type":"code","source":"# ============================================\n# CHUNK 4 ‚Äì reward functions, generate, evaluate\n# ============================================\nmatch_format = re.compile(\n    rf\"^[\\s]{{0,}}\"\n    rf\"{reasoning_start}.+?{reasoning_end}.*?\"\n    rf\"{solution_start}(.+?){solution_end}\"\n    rf\"[\\s]{{0,}}$\",\n    flags=re.MULTILINE | re.DOTALL,\n)\n\ndef match_format_exactly(prompts, completions, **kwargs):\n    return [\n        0 if match_format.search(response) is None else 3.0\n        for response in completions\n    ]\n\ndef match_format_approximately(prompts, completions, **kwargs):\n    scores = []\n    for completion in completions:\n        score = 0\n        response = completion\n        score += 0.5 if response.count(reasoning_start) == 1 else -0.5\n        score += 0.5 if response.count(reasoning_end) == 1 else -0.5\n        score += 0.5 if response.count(solution_start) == 1 else -0.5\n        score += 0.5 if response.count(solution_end) == 1 else -0.5\n        scores.append(score)\n    return scores\n\ndef check_answer(prompts, completions, answer, **kwargs):\n    responses = completions\n    extracted_responses = [\n        guess.group(1) if (guess := match_format.search(r)) is not None else None\n        for r in responses\n    ]\n    scores = []\n    assert len(extracted_responses) == len(answer)\n    for guess, true_answer in zip(extracted_responses, answer):\n        score = 0\n        if guess is None:\n            scores.append(0); continue\n        if guess == true_answer:\n            score += 3.0\n        elif guess.strip() == true_answer.strip():\n            score += 1.5\n        else:\n            try:\n                ratio = float(guess) / float(true_answer)\n                if 0.9 <= ratio <= 1.1:\n                    score += 0.5\n                elif 0.8 <= ratio <= 1.2:\n                    score += 0.25\n                else:\n                    score -= 1.0\n            except:\n                score -= 0.5\n        scores.append(score)\n    return scores\n\nmatch_numbers = re.compile(\n    rf\"{solution_start}.*?([\\d\\.]{{1,}})\", flags=re.MULTILINE | re.DOTALL\n)\n\ndef check_numbers(prompts, completions, answer, **kwargs):\n    question = kwargs[\"question\"]\n    responses = completions\n    extracted_responses = [\n        guess.group(1) if (guess := match_numbers.search(r)) is not None else None\n        for r in responses\n    ]\n    scores = []\n    for guess, true_answer in zip(extracted_responses, answer):\n        if guess is None:\n            scores.append(0); continue\n        try:\n            true_answer_f = float(true_answer.strip())\n            guess_f = float(guess.strip())\n            scores.append(1.5 if guess_f == true_answer_f else 0.0)\n        except:\n            scores.append(0)\n    return scores\n\ndef generate(question, sampler, temperature=0.7, top_k=50, top_p=0.95, seed=None):\n    if isinstance(question, str):\n        input_batch = [\n            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=question)\n        ]\n    else:\n        input_batch = [\n            TEMPLATE.format(system_prompt=SYSTEM_PROMPT, question=q)\n            for q in question\n        ]\n    out_data = sampler(\n        input_strings=input_batch,\n        max_generation_steps=768,\n        temperature=temperature,\n        top_k=top_k,\n        top_p=top_p,\n        echo=False,\n        seed=seed if seed is not None else None,\n        eos_tokens=[1, 106],\n    )\n    output = out_data.text\n    return output[0] if isinstance(question, str) else output\n\nfrom tqdm.auto import tqdm\n\ndef evaluate(\n    dataset,\n    sampler,\n    temperature=0.7,\n    top_k=50,\n    top_p=0.95,\n    num_passes=1,\n    corr_lst=False,\n    make_lst=False,\n):\n    response_lst = []\n    corr = partially_corr = corr_format = total = 0\n\n    for batch in tqdm(dataset):\n        answers = batch[\"answer\"]\n        questions = batch[\"question\"]\n\n        multiple_call_responses = [[] for _ in range(len(questions))]\n        for p in range(num_passes):\n            responses = generate(\n                questions, sampler, temperature, top_k, top_p, seed=p\n            )\n            for idx, response in enumerate(responses):\n                multiple_call_responses[idx].append(response)\n\n        for question, multiple_call_response, answer in zip(\n            questions, multiple_call_responses, answers\n        ):\n            corr_ctr_per_question = 0\n            partially_corr_per_question = 0\n            corr_format_per_question = 0\n\n            for response in multiple_call_response:\n                extracted_response = (\n                    guess.group(1)\n                    if (guess := match_numbers.search(response)) is not None\n                    else \"-1000000\"\n                )\n                try:\n                    if float(extracted_response.strip()) == float(answer.strip()):\n                        corr_ctr_per_question += 1\n                    ratio = float(extracted_response.strip()) / float(answer.strip())\n                    if 0.9 <= ratio <= 1.1:\n                        partially_corr_per_question += 1\n                except:\n                    pass\n\n                if match_format.search(response) is not None:\n                    corr_format_per_question += 1\n\n                if (corr_ctr_per_question > 0 and\n                    partially_corr_per_question > 0 and\n                    corr_format_per_question > 0):\n                    break\n\n            if corr_ctr_per_question > 0:\n                corr += 1\n                if corr_lst and make_lst:\n                    response_lst.append((question, answer, multiple_call_response))\n            else:\n                if not corr_lst and make_lst:\n                    response_lst.append((question, answer, multiple_call_response))\n            if partially_corr_per_question > 0:\n                partially_corr += 1\n            if corr_format_per_question > 0:\n                corr_format += 1\n\n            total += 1\n\n    to_return = (\n        corr,\n        total,\n        corr / total * 100,\n        partially_corr / total * 100,\n        corr_format / total * 100,\n    )\n    if make_lst:\n        return to_return, response_lst\n    return to_return\n\nsampler = sampler_lib.Sampler(\n    transformer=lora_policy,\n    tokenizer=tokenizer,\n    cache_config=sampler_lib.CacheConfig(\n        cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        num_layers=model_config.num_layers,\n        num_kv_heads=model_config.num_kv_heads,\n        head_dim=model_config.head_dim,\n    ),\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:52:12.425113Z","iopub.execute_input":"2025-11-23T20:52:12.425445Z","iopub.status.idle":"2025-11-23T20:52:12.509477Z","shell.execute_reply.started":"2025-11-23T20:52:12.425423Z","shell.execute_reply":"2025-11-23T20:52:12.508367Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"7. Training Setup: Optimizer, RLCluster, GRPOLearner ‚Äì CHUNK 5\n\nBuilds a learning-rate schedule:\n\nWarmup + cosine decay using optax.schedules.warmup_cosine_decay_schedule.\n\nLR ramps from 0 to LEARNING_RATE during WARMUP_STEPS.\n\nThen decays back to 0 over MAX_STEPS.\n\nCreates an AdamW optimizer:\n\nWith b1, b2, weight_decay as defined in the config.\n\nWraps with optax.clip_by_global_norm if MAX_GRAD_NORM is set.\n\nSets up checkpointing via CheckpointManagerOptions:\n\nSaves every SAVE_INTERVAL_STEPS.\n\nKeeps up to MAX_TO_KEEP latest checkpoints in CKPT_DIR.\n\nConfigures metrics logging:\n\nA MetricsLoggerOptions pointing to a TensorBoard directory.\n\nOptional W&B logging can be added later.\n\nBuilds RLTrainingConfig:\n\neval_every_n_steps: how often to evaluate.\n\nmax_steps: total number of GRPO updates.\n\nmini_batch_size and train_micro_batch_size: control gradient accumulation.\n\nactor_optimizer: the AdamW optimizer.\n\ncheckpoint_root_directory and checkpointing_options.\n\nConstructs the RLCluster:\n\nSpecifies mesh roles:\n\nRole.ACTOR: LoRA policy model (lora_policy).\n\nRole.REFERENCE: frozen base model (ref_model).\n\nRole.ROLLOUT: same mesh for generation.\n\nUses base_rollout.RolloutConfig to set:\n\nmax_prompt_length, max_tokens_to_generate, cache size.\n\nSampling parameters: TEMPERATURE, TOP_P, TOP_K, EOS tokens.\n\nCreates a GRPOConfig with:\n\nnum_generations (G), num_iterations, beta, epsilon.\n\nFinally, instantiates the GRPOLearner:\n\ngrpo_trainer = GRPOLearner(...)\n\nPasses:\n\nrl_cluster.\n\nList of reward functions:\n\nmatch_format_exactly, match_format_approximately,\ncheck_answer, check_numbers.\n\ngrpo_config.\n\nTo train:\n\nCall grpo_trainer.train(train_dataset) inside a with mesh: context.\n\nGRPO will:\n\nSample multiple responses per prompt.\n\nScore them with the reward functions.\n\nCompute group-relative advantages.\n\nUpdate LoRA parameters via PPO-style clipped loss.","metadata":{}},{"cell_type":"code","source":"# CHUNK 5 ‚Äì Training config, RLCluster, GRPOLearner, training loop\n\nimport optax\nfrom orbax import checkpoint as ocp\nfrom tunix.sft import metrics_logger\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\n\n# --------------------------------------------------------------------\n# 1) Checkpoint + metrics config\n# --------------------------------------------------------------------\n\ncheckpointing_options = ocp.CheckpointManagerOptions(\n    save_interval_steps=SAVE_INTERVAL_STEPS,\n    max_to_keep=MAX_TO_KEEP,\n)\n\nmetrics_logging_options = metrics_logger.MetricsLoggerOptions(\n    log_dir=\"/tmp/content/tmp/tensorboard/grpo\",\n    flush_every_n_steps=20,\n)\n\n# --------------------------------------------------------------------\n# 2) Optimizer, LR schedule, gradient clipping\n# --------------------------------------------------------------------\n\nlr_schedule = optax.schedules.warmup_cosine_decay_schedule(\n    init_value=0.0,\n    peak_value=LEARNING_RATE,\n    warmup_steps=WARMUP_STEPS,\n    decay_steps=MAX_STEPS,\n    end_value=0.0,\n)\n\noptimizer = optax.adamw(\n    learning_rate=lr_schedule,\n    b1=B1,\n    b2=B2,\n    weight_decay=WEIGHT_DECAY,\n)\n\nif MAX_GRAD_NORM is not None:\n    optimizer = optax.chain(\n        optax.clip_by_global_norm(MAX_GRAD_NORM),\n        optimizer,\n    )\n\n# --------------------------------------------------------------------\n# 3) RLTrainingConfig + ClusterConfig\n# --------------------------------------------------------------------\n\ntraining_config = rl_cluster_lib.RLTrainingConfig(\n    actor_optimizer=optimizer,\n    eval_every_n_steps=EVAL_EVERY_N_STEPS,\n    max_steps=MAX_STEPS,\n    mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n    train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n    # logging (set to None to fully disable logs)\n    metrics_logging_options=None,  # or metrics_logging_options\n    # checkpoint saving\n    checkpoint_root_directory=CKPT_DIR,\n    checkpointing_options=checkpointing_options,\n)\n\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={\n        rl_cluster_lib.Role.ACTOR: mesh,\n        rl_cluster_lib.Role.REFERENCE: mesh,\n        rl_cluster_lib.Role.ROLLOUT: mesh,\n    },\n    rollout_engine=\"vanilla\",\n    offload_to_cpu=False,\n    training_config=training_config,\n    rollout_config=base_rollout.RolloutConfig(\n        max_tokens_to_generate=TOTAL_GENERATION_STEPS,\n        max_prompt_length=MAX_PROMPT_LENGTH,\n        kv_cache_size=MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256,\n        temperature=TEMPERATURE,\n        top_p=TOP_P,\n        top_k=TOP_K,\n        eos_tokens=[1, 106],\n    ),\n)\n\n# --------------------------------------------------------------------\n# 4) GRPOConfig (algorithm config)\n# --------------------------------------------------------------------\n\ngrpo_config = GRPOConfig(\n    num_generations=NUM_GENERATIONS,\n    num_iterations=NUM_ITERATIONS,\n    beta=BETA,\n    epsilon=EPSILON,\n)\n\n# --------------------------------------------------------------------\n# 5) RLCluster ‚Äì ties actor, reference, tokenizer + config together\n# --------------------------------------------------------------------\n\nrl_cluster = rl_cluster_lib.RLCluster(\n    actor=lora_policy,    # LoRA policy model\n    reference=ref_model,  # frozen reference model\n    tokenizer=tokenizer,\n    cluster_config=cluster_config,\n)\n\n# --------------------------------------------------------------------\n# 6) GRPOLearner (trainer) with reward functions\n# --------------------------------------------------------------------\n\ngrpo_trainer = GRPOLearner(\n    rl_cluster=rl_cluster,\n    reward_fns=[\n        match_format_exactly,\n        match_format_approximately,\n        check_answer,\n        check_numbers,\n    ],\n    grpo_config=grpo_config,\n)\n\n# --------------------------------------------------------------------\n# 7) Train ‚Äì GRPO loop on GSM8K train_dataset\n# --------------------------------------------------------------------\n\nwith mesh:\n    grpo_trainer.train(train_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-23T20:52:18.119049Z","iopub.execute_input":"2025-11-23T20:52:18.119339Z"}},"outputs":[{"name":"stderr","text":"WARNING:absl:Reference model and actor model are colocated but do not share the same backbone. This will result in an unnecessary model copy and increased HBM usage.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Actor Training:   0%|          | 0/3738 [00:00<?, ?step/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31b3279686c24624b53290d55d0fee99"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"8. Evaluation & Checkpoint Reload ‚Äì CHUNK 6 & 7\n\nBuilds a Sampler object:\n\nWraps lora_policy and tokenizer.\n\nUses a KV-cache sized for MAX_PROMPT_LENGTH + TOTAL_GENERATION_STEPS + 256.\n\nDefines a generate() helper:\n\nGiven one or many questions:\n\nFormats them with TEMPLATE + SYSTEM_PROMPT.\n\nCalls the sampler with specific decoding config (greedy / standard / liberal).\n\nReturns generated text(s).\n\nDefines an evaluate() function:\n\nLoops through the test dataset.\n\nFor each question:\n\nGenerates multiple responses (num_passes).\n\nExtracts numeric answers using the same regex as reward functions.\n\nTracks:\n\nExact accuracy.\n\nPartial accuracy (ratio in [0.9, 1.1]).\n\nFormat accuracy (correct tags).\n\nReturns (corr, total, accuracy, partial_accuracy, format_accuracy).\n\nPre-training evaluation:\n\nRuns evaluate(test_dataset, sampler, **GENERATION_CONFIGS[\"greedy\"]) on the base LoRA model.\n\nProvides baseline percentages before GRPO.\n\nPost-training evaluation:\n\nLocates the latest checkpoint in CKPT_DIR/actor/STEP/model_params.\n\nRestores LoRA parameters into lora_policy using orbax and nnx.update.\n\nRebuilds the sampler with the updated policy.\n\nRuns evaluate(...) again and prints new accuracy metrics.\n\nIf training worked, you should see:\n\nHigher accuracy and partial_accuracy.\n\nMuch better format_accuracy (the model follows the <reasoning> / <answer> pattern more reliably).","metadata":{}},{"cell_type":"markdown","source":"9. How to Use This in Kaggle\n\nRun cells in order:\n\nCHUNK 0 once, then Restart the notebook.\n\nRun CHUNK 1‚Äì4 to:\n\nVerify TPU.\n\nLoad Gemma3 model & tokenizer.\n\nPrepare GSM8K data.\n\nDefine reward functions.\n\nOptionally run the pre-training evaluation.\n\nRun CHUNK 5 to start GRPO training on TPU (this takes time).\n\nRun final evaluation cells to see improvement in reasoning metrics.\n\nAt the end you‚Äôll have:\n\nA checkpointed LoRA policy tuned for math reasoning.\n\nA notebook ready to adapt to your own datasets or competitions.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}