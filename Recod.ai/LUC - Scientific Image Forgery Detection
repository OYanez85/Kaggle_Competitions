{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":113558,"databundleVersionId":14456136,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# Imports ONLY (no pip install here)\n# ============================================================\nimport os\nimport gc\nimport json\nimport random\nfrom glob import glob\nfrom pathlib import Path\n\nimport cv2\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\n# Albumentations is already installed in Kaggle image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n# ============================================================\n# Config\n# ============================================================\nclass CFG:\n    seed = 42\n    debug = False\n    img_size = 512\n    batch_size = 4\n    num_workers = 4\n    epochs = 12\n    lr = 1e-4\n    weight_decay = 1e-5\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    dataset_dir = \"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"\n    min_pixels_for_forgery = 50\n    save_path = \"/kaggle/working/recodai_unet_baseline.pth\"\n\nprint(\"Using device:\", CFG.device)\n\ndef seed_everything(seed: int = 42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = False\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(CFG.seed)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T19:00:21.919371Z","iopub.execute_input":"2025-12-03T19:00:21.919939Z","iopub.status.idle":"2025-12-03T19:00:27.900115Z","shell.execute_reply.started":"2025-12-03T19:00:21.919909Z","shell.execute_reply":"2025-12-03T19:00:27.899299Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================\n# Build training dataframe (train + supplemental)\n# ============================================================\nDATA_DIR = CFG.dataset_dir\nTRAIN_IMG_DIR = os.path.join(DATA_DIR, \"train_images\")\nTRAIN_MASK_DIR = os.path.join(DATA_DIR, \"train_masks\")\nSUPP_IMG_DIR = os.path.join(DATA_DIR, \"supplemental_images\")\nSUPP_MASK_DIR = os.path.join(DATA_DIR, \"supplemental_masks\")\n\n# collect images both from flat and nested folders (authentic/forged)\ntrain_img_paths = glob(os.path.join(TRAIN_IMG_DIR, \"*.png\"))\ntrain_img_paths += glob(os.path.join(TRAIN_IMG_DIR, \"*\", \"*.png\"))\n\nif os.path.exists(SUPP_IMG_DIR):\n    supp_paths = glob(os.path.join(SUPP_IMG_DIR, \"*.png\"))\n    supp_paths += glob(os.path.join(SUPP_IMG_DIR, \"*\", \"*.png\"))\n    train_img_paths += supp_paths\n\nrecords = []\nfor img_path in train_img_paths:\n    case_id = os.path.splitext(os.path.basename(img_path))[0]\n\n    mask_path = None\n    m1 = os.path.join(TRAIN_MASK_DIR, case_id + \".npy\")\n    m2 = os.path.join(SUPP_MASK_DIR, case_id + \".npy\")\n\n    if os.path.exists(m1):\n        mask_path = m1\n    elif os.path.exists(m2):\n        mask_path = m2\n\n    has_mask = 1 if mask_path is not None else 0\n    records.append(\n        {\n            \"case_id\": case_id,\n            \"image_path\": img_path,\n            \"mask_path\": mask_path,\n            \"has_mask\": has_mask,\n        }\n    )\n\ndf = pd.DataFrame(records)\ndf = df.drop_duplicates(subset=[\"case_id\"]).reset_index(drop=True)\n\nprint(\"Total train images:\", len(df))\nprint(\"Forgery images   :\", df[\"has_mask\"].sum())\nprint(\"Authentic images :\", (df[\"has_mask\"] == 0).sum())\n\nif CFG.debug:\n    df = df.sample(200, random_state=CFG.seed).reset_index(drop=True)\n\n\n# ============================================================\n# Train / validation split (stratified by has_mask)\n# ============================================================\ntrain_idx, valid_idx = train_test_split(\n    df.index,\n    test_size=0.15,\n    random_state=CFG.seed,\n    stratify=df[\"has_mask\"],\n)\n\ndf[\"is_valid\"] = False\ndf.loc[valid_idx, \"is_valid\"] = True\n\nprint(\"Train size:\", (~df[\"is_valid\"]).sum())\nprint(\"Valid size:\", df[\"is_valid\"].sum())\n\n\n# ============================================================\n# Albumentations transforms\n# ============================================================\ndef get_train_transforms():\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size, CFG.img_size),\n            A.HorizontalFlip(p=0.5),\n            A.VerticalFlip(p=0.5),\n            A.RandomRotate90(p=0.5),\n            A.ShiftScaleRotate(\n                shift_limit=0.05, scale_limit=0.15, rotate_limit=15,\n                p=0.5, border_mode=cv2.BORDER_CONSTANT\n            ),\n            A.RandomBrightnessContrast(p=0.5),\n            A.GaussNoise(p=0.2),\n            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n            ToTensorV2(),\n        ]\n    )\n\n\ndef get_valid_transforms():\n    return A.Compose(\n        [\n            A.Resize(CFG.img_size, CFG.img_size),\n            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n            ToTensorV2(),\n        ]\n    )\n\n\n# ============================================================\n# Dataset\n# ============================================================\nclass ForgeryDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, transforms=None):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = row[\"image_path\"]\n        mask_path = row[\"mask_path\"]\n\n        # Read image (BGR -> RGB)\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        h, w, _ = image.shape\n\n        # Build mask (H, W)\n        if mask_path is not None and os.path.exists(mask_path):\n            mask_raw = np.load(mask_path)  # shape: (N, H, W) or (C, H, W)\n            if mask_raw.ndim == 3:\n                # union across channels / instances\n                mask = np.any(mask_raw, axis=0).astype(np.uint8)\n            elif mask_raw.ndim == 2:\n                mask = mask_raw.astype(np.uint8)\n            else:\n                raise ValueError(f\"Unexpected mask shape {mask_raw.shape}\")\n        else:\n            mask = np.zeros((h, w), dtype=np.uint8)\n\n        # Albumentations expects mask as (H, W)\n        if self.transforms is not None:\n            augmented = self.transforms(image=image, mask=mask)\n            image = augmented[\"image\"]   # torch tensor\n            mask = augmented[\"mask\"]     # torch tensor\n\n        # Ensure mask shape is (1, H, W) and dtype float32\n        if isinstance(mask, np.ndarray):\n            if mask.ndim == 2:\n                mask = mask[None, :, :]\n            mask = mask.astype(np.float32)\n        else:\n            # torch tensor\n            if mask.ndim == 2:\n                mask = mask.unsqueeze(0)\n            mask = mask.float()\n\n        return image, mask\n\n\n# ============================================================\n# DataLoaders\n# ============================================================\ntrain_dataset = ForgeryDataset(df[df[\"is_valid\"] == False], transforms=get_train_transforms())\nvalid_dataset = ForgeryDataset(df[df[\"is_valid\"] == True], transforms=get_valid_transforms())\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=CFG.batch_size,\n    shuffle=True,\n    num_workers=CFG.num_workers,\n    pin_memory=True,\n    drop_last=True,\n)\n\nvalid_loader = DataLoader(\n    valid_dataset,\n    batch_size=CFG.batch_size,\n    shuffle=False,\n    num_workers=CFG.num_workers,\n    pin_memory=True,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T19:09:33.556215Z","iopub.execute_input":"2025-12-03T19:09:33.556569Z","iopub.status.idle":"2025-12-03T19:09:35.714369Z","shell.execute_reply.started":"2025-12-03T19:09:33.556519Z","shell.execute_reply":"2025-12-03T19:09:35.713525Z"}},"outputs":[{"name":"stdout","text":"Total train images: 2795\nForgery images   : 2795\nAuthentic images : 0\nTrain size: 2375\nValid size: 420\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n  original_init(self, **validated_kwargs)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================\n# Simple U-Net in pure PyTorch\n# ============================================================\nclass DoubleConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, 3, padding=1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Down(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.MaxPool2d(2),\n            DoubleConv(in_channels, out_channels),\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\n\nclass Up(nn.Module):\n    def __init__(self, in_channels, out_channels, bilinear=True):\n        super().__init__()\n        if bilinear:\n            self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n            self.conv = DoubleConv(in_channels, out_channels)\n        else:\n            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2, 2, stride=2)\n            self.conv = DoubleConv(in_channels, out_channels)\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n        x1 = nn.functional.pad(\n            x1,\n            [diffX // 2, diffX - diffX // 2, diffY // 2, diffY - diffY // 2],\n        )\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n\n\nclass OutConv(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, 1)\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass UNet(nn.Module):\n    def __init__(self, n_channels=3, n_classes=1, bilinear=True):\n        super().__init__()\n        self.inc = DoubleConv(n_channels, 64)\n        self.down1 = Down(64, 128)\n        self.down2 = Down(128, 256)\n        self.down3 = Down(256, 512)\n        factor = 2 if bilinear else 1\n        self.down4 = Down(512, 1024 // factor)\n        self.up1 = Up(1024, 512 // factor, bilinear)\n        self.up2 = Up(512, 256 // factor, bilinear)\n        self.up3 = Up(256, 128 // factor, bilinear)\n        self.up4 = Up(128, 64, bilinear)\n        self.outc = OutConv(64, n_classes)\n\n    def forward(self, x):\n        x1 = self.inc(x)\n        x2 = self.down1(x1)\n        x3 = self.down2(x2)\n        x4 = self.down3(x3)\n        x5 = self.down4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        return self.outc(x)\n\n\ndef get_model():\n    return UNet(n_channels=3, n_classes=1)\n\n\n# ============================================================\n# Loss: BCE + Dice\n# ============================================================\nclass DiceLoss(nn.Module):\n    def __init__(self, smooth=1.0):\n        super().__init__()\n        self.smooth = smooth\n\n    def forward(self, logits, targets):\n        probs = torch.sigmoid(logits)\n        probs = probs.view(probs.size(0), -1)\n        targets = targets.view(targets.size(0), -1)\n        intersection = (probs * targets).sum(dim=1)\n        union = probs.sum(dim=1) + targets.sum(dim=1)\n        dice = (2 * intersection + self.smooth) / (union + self.smooth)\n        return 1 - dice.mean()\n\n\nbce_loss_fn = nn.BCEWithLogitsLoss()\ndice_loss_fn = DiceLoss()\n\ndef criterion(logits, targets):\n    return bce_loss_fn(logits, targets) + dice_loss_fn(logits, targets)\n\n\n# ============================================================\n# Training / validation loops\n# ============================================================\ndef train_one_epoch(model, loader, optimizer, epoch):\n    model.train()\n    running_loss = 0.0\n    for step, (images, masks) in enumerate(loader):\n        images = images.to(CFG.device)\n        masks = masks.to(CFG.device)\n\n        optimizer.zero_grad()\n        logits = model(images)\n        loss = criterion(logits, masks)\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if (step + 1) % 100 == 0:\n            print(f\"Epoch {epoch} Step {step+1}/{len(loader)} Loss {running_loss/(step+1):.4f}\")\n    return running_loss / len(loader)\n\n\ndef valid_one_epoch(model, loader, epoch):\n    model.eval()\n    running_loss = 0.0\n    dice_scores = []\n\n    with torch.no_grad():\n        for images, masks in loader:\n            images = images.to(CFG.device)\n            masks = masks.to(CFG.device)\n\n            logits = model(images)\n            loss = criterion(logits, masks)\n            running_loss += loss.item()\n\n            probs = torch.sigmoid(logits)\n            preds = (probs > 0.5).float()\n\n            intersection = (preds * masks).sum(dim=(1, 2, 3))\n            union = preds.sum(dim=(1, 2, 3)) + masks.sum(dim=(1, 2, 3))\n            dice = (2 * intersection + 1.0) / (union + 1.0)\n            dice_scores.extend(dice.cpu().numpy().tolist())\n\n    val_loss = running_loss / len(loader)\n    mean_dice = float(np.mean(dice_scores)) if dice_scores else 0.0\n    print(f\"Epoch {epoch} | Val Loss: {val_loss:.4f} | Val Dice: {mean_dice:.4f}\")\n    return val_loss, mean_dice\n\n\n# ============================================================\n# Train\n# ============================================================\nmodel = get_model().to(CFG.device)\noptimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\n\nbest_val = np.inf\nbest_dice = 0.0\n\nfor epoch in range(1, CFG.epochs + 1):\n    train_loss = train_one_epoch(model, train_loader, optimizer, epoch)\n    val_loss, val_dice = valid_one_epoch(model, valid_loader, epoch)\n\n    if val_loss < best_val:\n        best_val = val_loss\n        best_dice = val_dice\n        torch.save(model.state_dict(), CFG.save_path)\n        print(f\"  ✅ Saved new best model (val_loss={best_val:.4f}, dice={best_dice:.4f})\")\n\nprint(\"Training done. Best val_loss:\", best_val, \"Best dice:\", best_dice)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T19:09:46.824229Z","iopub.execute_input":"2025-12-03T19:09:46.825053Z","iopub.status.idle":"2025-12-03T20:37:28.429181Z","shell.execute_reply.started":"2025-12-03T19:09:46.825024Z","shell.execute_reply":"2025-12-03T20:37:28.428291Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Step 100/593 Loss 1.4205\nEpoch 1 Step 200/593 Loss 1.3465\nEpoch 1 Step 300/593 Loss 1.3060\nEpoch 1 Step 400/593 Loss 1.2818\nEpoch 1 Step 500/593 Loss 1.2628\nEpoch 1 | Val Loss: 1.1557 | Val Dice: 0.0581\n  ✅ Saved new best model (val_loss=1.1557, dice=0.0581)\nEpoch 2 Step 100/593 Loss 1.1513\nEpoch 2 Step 200/593 Loss 1.1395\nEpoch 2 Step 300/593 Loss 1.1336\nEpoch 2 Step 400/593 Loss 1.1320\nEpoch 2 Step 500/593 Loss 1.1278\nEpoch 2 | Val Loss: 1.1360 | Val Dice: 0.1098\n  ✅ Saved new best model (val_loss=1.1360, dice=0.1098)\nEpoch 3 Step 100/593 Loss 1.1083\nEpoch 3 Step 200/593 Loss 1.1034\nEpoch 3 Step 300/593 Loss 1.1008\nEpoch 3 Step 400/593 Loss 1.1023\nEpoch 3 Step 500/593 Loss 1.1005\nEpoch 3 | Val Loss: 1.1433 | Val Dice: 0.1505\nEpoch 4 Step 100/593 Loss 1.0744\nEpoch 4 Step 200/593 Loss 1.0700\nEpoch 4 Step 300/593 Loss 1.0708\nEpoch 4 Step 400/593 Loss 1.0656\nEpoch 4 Step 500/593 Loss 1.0647\nEpoch 4 | Val Loss: 1.1052 | Val Dice: 0.1161\n  ✅ Saved new best model (val_loss=1.1052, dice=0.1161)\nEpoch 5 Step 100/593 Loss 1.0554\nEpoch 5 Step 200/593 Loss 1.0530\nEpoch 5 Step 300/593 Loss 1.0581\nEpoch 5 Step 400/593 Loss 1.0557\nEpoch 5 Step 500/593 Loss 1.0559\nEpoch 5 | Val Loss: 1.0966 | Val Dice: 0.1931\n  ✅ Saved new best model (val_loss=1.0966, dice=0.1931)\nEpoch 6 Step 100/593 Loss 1.0193\nEpoch 6 Step 200/593 Loss 1.0234\nEpoch 6 Step 300/593 Loss 1.0286\nEpoch 6 Step 400/593 Loss 1.0356\nEpoch 6 Step 500/593 Loss 1.0370\nEpoch 6 | Val Loss: 1.0733 | Val Dice: 0.1454\n  ✅ Saved new best model (val_loss=1.0733, dice=0.1454)\nEpoch 7 Step 100/593 Loss 1.0474\nEpoch 7 Step 200/593 Loss 1.0344\nEpoch 7 Step 300/593 Loss 1.0359\nEpoch 7 Step 400/593 Loss 1.0423\nEpoch 7 Step 500/593 Loss 1.0358\nEpoch 7 | Val Loss: 1.0186 | Val Dice: 0.2057\n  ✅ Saved new best model (val_loss=1.0186, dice=0.2057)\nEpoch 8 Step 100/593 Loss 1.0257\nEpoch 8 Step 200/593 Loss 1.0072\nEpoch 8 Step 300/593 Loss 1.0103\nEpoch 8 Step 400/593 Loss 1.0135\nEpoch 8 Step 500/593 Loss 1.0143\nEpoch 8 | Val Loss: 1.0156 | Val Dice: 0.2017\n  ✅ Saved new best model (val_loss=1.0156, dice=0.2017)\nEpoch 9 Step 100/593 Loss 1.0125\nEpoch 9 Step 200/593 Loss 1.0151\nEpoch 9 Step 300/593 Loss 1.0130\nEpoch 9 Step 400/593 Loss 1.0051\nEpoch 9 Step 500/593 Loss 1.0072\nEpoch 9 | Val Loss: 1.0164 | Val Dice: 0.2527\nEpoch 10 Step 100/593 Loss 1.0195\nEpoch 10 Step 200/593 Loss 1.0060\nEpoch 10 Step 300/593 Loss 1.0069\nEpoch 10 Step 400/593 Loss 1.0089\nEpoch 10 Step 500/593 Loss 1.0037\nEpoch 10 | Val Loss: 1.0382 | Val Dice: 0.1821\nEpoch 11 Step 100/593 Loss 1.0141\nEpoch 11 Step 200/593 Loss 1.0059\nEpoch 11 Step 300/593 Loss 1.0050\nEpoch 11 Step 400/593 Loss 1.0065\nEpoch 11 Step 500/593 Loss 0.9992\nEpoch 11 | Val Loss: 0.9846 | Val Dice: 0.2373\n  ✅ Saved new best model (val_loss=0.9846, dice=0.2373)\nEpoch 12 Step 100/593 Loss 1.0067\nEpoch 12 Step 200/593 Loss 0.9960\nEpoch 12 Step 300/593 Loss 0.9818\nEpoch 12 Step 400/593 Loss 0.9834\nEpoch 12 Step 500/593 Loss 0.9859\nEpoch 12 | Val Loss: 1.0140 | Val Dice: 0.2072\nTraining done. Best val_loss: 0.9846286983717055 Best dice: 0.23734355180423725\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================\n# RLE encoding (Recod.ai format compatible)\n# ============================================================\ndef rle_encode_mask(mask: np.ndarray, fg_val: int = 1):\n    \"\"\"\n    mask: 2D numpy array of type uint8 {0,1}\n    Returns: list[int] -> [start_1, len_1, start_2, len_2, ...]\n    1-based indexing, row-major order.\n    \"\"\"\n    assert mask.ndim == 2\n    pixels = (mask == fg_val).astype(np.uint8).flatten(order=\"C\")\n\n    # pad with zeros at start and end to catch transitions\n    pixels = np.concatenate([[0], pixels, [0]])\n    changes = np.where(pixels[1:] != pixels[:-1])[0] + 1\n\n    # starts & lengths\n    starts = changes[0::2]\n    ends = changes[1::2]\n    lengths = ends - starts\n\n    rle = []\n    for s, l in zip(starts, lengths):\n        rle.extend([int(s), int(l)])\n    return rle\n\n\n# ============================================================\n# Load best model for inference\n# ============================================================\nmodel = get_model().to(CFG.device)\nmodel.load_state_dict(torch.load(CFG.save_path, map_location=CFG.device))\nmodel.eval()\nprint(\"Loaded best model from:\", CFG.save_path)\n\n\n# ============================================================\n# Test dataset / loader\n# ============================================================\nTEST_IMG_DIR = os.path.join(DATA_DIR, \"test_images\")\ntest_img_paths = glob(os.path.join(TEST_IMG_DIR, \"*.png\"))\nprint(\"Test images:\", len(test_img_paths))\n\n# Map case_id -> image path\ntest_case_to_path = {\n    os.path.splitext(os.path.basename(p))[0]: p for p in test_img_paths\n}\n\nsample_sub = pd.read_csv(os.path.join(DATA_DIR, \"sample_submission.csv\"))\nprint(sample_sub.head())\n\n\nclass TestDataset(Dataset):\n    def __init__(self, df, transforms=None):\n        self.df = df.reset_index(drop=True)\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        case_id = str(row[\"case_id\"])\n        img_path = test_case_to_path[case_id]\n\n        image = cv2.imread(img_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        if self.transforms is not None:\n            augmented = self.transforms(image=image)\n            image = augmented[\"image\"]\n\n        return case_id, image\n\n\ntest_dataset = TestDataset(sample_sub, transforms=get_valid_transforms())\ntest_loader = DataLoader(\n    test_dataset,\n    batch_size=1,   # safer for variable sizes; you can bump later\n    shuffle=False,\n    num_workers=2,\n    pin_memory=True,\n)\n\n\n# ============================================================\n# Inference and submission\n# ============================================================\ndef predict_mask_single(image_tensor):\n    \"\"\"\n    image_tensor: (3, H, W) torch tensor normalized\n    returns: binary mask (H, W) uint8\n    \"\"\"\n    with torch.no_grad():\n        x = image_tensor.unsqueeze(0).to(CFG.device)\n        logits = model(x)\n        probs = torch.sigmoid(logits)[0, 0].cpu().numpy()\n    # resize back to original image size is not needed here because we resized to fixed size\n    mask = (probs > 0.5).astype(np.uint8)\n    return mask\n\n\npredictions = []\n\nfor case_id, image in test_loader:\n    case_id = case_id[0]  # batch size 1\n    image = image[0]      # (3, H, W)\n\n    mask = predict_mask_single(image)\n\n    # Post-process & threshold\n    num_fg = mask.sum()\n    if num_fg < CFG.min_pixels_for_forgery:\n        annotation = \"authentic\"\n    else:\n        rle = rle_encode_mask(mask)\n        # according to public notebooks, rle_encode from metric returns a list\n        # which is JSON-serialized for submission\n        annotation = json.dumps([int(x) for x in rle])\n\n    predictions.append({\"case_id\": case_id, \"annotation\": annotation})\n\nsubmission = pd.DataFrame(predictions)\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-03T20:37:31.681823Z","iopub.execute_input":"2025-12-03T20:37:31.682570Z","iopub.status.idle":"2025-12-03T20:37:32.113074Z","shell.execute_reply.started":"2025-12-03T20:37:31.682540Z","shell.execute_reply":"2025-12-03T20:37:32.112160Z"}},"outputs":[{"name":"stdout","text":"Loaded best model from: /kaggle/working/recodai_unet_baseline.pth\nTest images: 1\n   case_id annotation\n0       45  authentic\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"  case_id                                         annotation\n0      45  [40572, 2, 95525, 2, 96036, 9, 96548, 9, 97060...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>case_id</th>\n      <th>annotation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>45</td>\n      <td>[40572, 2, 95525, 2, 96036, 9, 96548, 9, 97060...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":10}]}