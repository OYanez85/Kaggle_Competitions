{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116062,"databundleVersionId":14084779,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:49:02.056789Z","iopub.execute_input":"2025-11-18T18:49:02.057109Z","iopub.status.idle":"2025-11-18T18:49:04.030355Z","shell.execute_reply.started":"2025-11-18T18:49:02.057068Z","shell.execute_reply":"2025-11-18T18:49:04.029467Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Imports & config","metadata":{}},{"cell_type":"code","source":"# CHUNK 1: Imports & global config\n\nimport os\nimport gc\nimport math\nfrom collections import Counter, defaultdict\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.metrics import f1_score\n\nDATA_DIR = \"/kaggle/input/cafa-6-protein-function-prediction\"\n\n# File paths (match your directory structure)\ntrain_fasta_path    = os.path.join(DATA_DIR, \"Train\", \"train_sequences.fasta\")\ntrain_terms_path    = os.path.join(DATA_DIR, \"Train\", \"train_terms.tsv\")\ntrain_taxonomy_path = os.path.join(DATA_DIR, \"Train\", \"train_taxonomy.tsv\")\ngo_obo_path         = os.path.join(DATA_DIR, \"Train\", \"go-basic.obo\")\nia_path             = os.path.join(DATA_DIR, \"IA.tsv\")\n\ntest_fasta_path     = os.path.join(DATA_DIR, \"Test\", \"testsuperset.fasta\")\ntest_taxon_path     = os.path.join(DATA_DIR, \"Test\", \"testsuperset-taxon-list.tsv\")\n\nsample_sub_path     = os.path.join(DATA_DIR, \"sample_submission.tsv\")\n\n# How many GO terms to model (by frequency rank)\nTOP_K_TERMS = 500\n\n# Minimum probability to output in submission\nMIN_PRED_PROB = 0.01\n\nRANDOM_STATE = 42\nnp.random.seed(RANDOM_STATE)\n\nprint(\"DATA_DIR contents:\", os.listdir(DATA_DIR))\nprint(\"Train folder:\", os.listdir(os.path.join(DATA_DIR, \"Train\")))\nprint(\"Test folder:\", os.listdir(os.path.join(DATA_DIR, \"Test\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:49:06.349455Z","iopub.execute_input":"2025-11-18T18:49:06.349958Z","iopub.status.idle":"2025-11-18T18:49:07.159281Z","shell.execute_reply.started":"2025-11-18T18:49:06.349930Z","shell.execute_reply":"2025-11-18T18:49:07.158441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nfor path in [\n    train_fasta_path,\n    train_terms_path,\n    train_taxonomy_path,\n    go_obo_path,\n    ia_path,\n    test_fasta_path,\n    test_taxon_path,\n    sample_sub_path,\n]:\n    print(path, \"OK\" if os.path.exists(path) else \"MISSING\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:49:13.387209Z","iopub.execute_input":"2025-11-18T18:49:13.388072Z","iopub.status.idle":"2025-11-18T18:49:13.400118Z","shell.execute_reply.started":"2025-11-18T18:49:13.388041Z","shell.execute_reply":"2025-11-18T18:49:13.399093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FASTA reader & basic loaders","metadata":{}},{"cell_type":"code","source":"# CHUNK 2: Utilities to read FASTA and load training data\n\ndef read_fasta(fasta_path):\n    \"\"\"\n    Read a FASTA file into a dict:\n    {protein_id: sequence}\n    \"\"\"\n    sequences = {}\n    current_id = None\n    current_seq = []\n\n    with open(fasta_path, \"r\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                continue\n\n            if line.startswith(\">\"):\n                # Save previous sequence if any\n                if current_id is not None:\n                    sequences[current_id] = \"\".join(current_seq)\n\n                # Example header: sp|P9WHI7|RECN_MYCT ...\n                header = line[1:]\n                parts = header.split(\"|\")\n                if len(parts) >= 2:\n                    prot_id = parts[1]\n                else:\n                    # Fallback: use whole header if unexpected format\n                    prot_id = header.split()[0]\n\n                current_id = prot_id\n                current_seq = []\n            else:\n                current_seq.append(line)\n\n        # Last sequence\n        if current_id is not None:\n            sequences[current_id] = \"\".join(current_seq)\n\n    return sequences\n\n\n# Load train sequences\ntrain_sequences = read_fasta(train_fasta_path)\nprint(f\"Loaded {len(train_sequences)} training sequences\")\n\n# Load training GO terms (labels)\ntrain_terms_df = pd.read_csv(\n    train_terms_path,\n    sep=\"\\t\",\n    header=None,\n    names=[\"protein_id\", \"go_term\", \"ontology\"]\n)\n\nprint(train_terms_df.head())\nprint(\"Unique GO terms in training:\", train_terms_df[\"go_term\"].nunique())\n\n# Load IA weights\nia_df = pd.read_csv(\n    ia_path,\n    sep=\"\\t\",\n    header=None,\n    names=[\"go_term\", \"ia\"]\n)\nia_map = dict(zip(ia_df[\"go_term\"], ia_df[\"ia\"]))\nprint(f\"Loaded IA weights for {len(ia_map)} GO terms\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:49:18.340446Z","iopub.execute_input":"2025-11-18T18:49:18.341166Z","iopub.status.idle":"2025-11-18T18:49:19.929167Z","shell.execute_reply.started":"2025-11-18T18:49:18.341135Z","shell.execute_reply":"2025-11-18T18:49:19.928261Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Build multi-label targets & keep only top-K GO terms","metadata":{}},{"cell_type":"code","source":"# CHUNK 3: Build multi-label targets and restrict to top-K GO terms\n\n# Aggregate GO terms per protein\nprot_to_terms = defaultdict(set)\nfor row in train_terms_df.itertuples(index=False):\n    prot_to_terms[row.protein_id].add(row.go_term)\n\nprint(\"Proteins with labels:\", len(prot_to_terms))\n\n# Count GO term frequencies\nterm_counts = Counter(train_terms_df[\"go_term\"])\n\n# Select top-K most frequent GO terms\nmost_common_terms = [t for t, _ in term_counts.most_common(TOP_K_TERMS)]\nmost_common_set = set(most_common_terms)\nprint(f\"Using TOP_K_TERMS={TOP_K_TERMS}, \"\n      f\"kept {len(most_common_terms)} terms (of {len(term_counts)} total).\")\n\n# Filter labels to those top terms\nfiltered_prot_to_terms = {\n    pid: list({t for t in terms if t in most_common_set})\n    for pid, terms in prot_to_terms.items()\n}\n\n# Only keep proteins that still have at least one label after filtering\nfiltered_prot_to_terms = {\n    pid: terms for pid, terms in filtered_prot_to_terms.items() if len(terms) > 0\n}\n\nprint(\"Proteins with at least 1 top-K label:\", len(filtered_prot_to_terms))\n\n# Create DataFrame aligned with sequences\ntrain_ids = []\ntrain_seqs = []\ntrain_labels = []\n\nfor pid, seq in train_sequences.items():\n    if pid in filtered_prot_to_terms:\n        train_ids.append(pid)\n        train_seqs.append(seq)\n        train_labels.append(filtered_prot_to_terms[pid])\n\ntrain_df = pd.DataFrame(\n    {\"protein_id\": train_ids, \"sequence\": train_seqs, \"labels\": train_labels}\n)\n\nprint(train_df.head())\nprint(\"Train_df shape:\", train_df.shape)\n\n# Binarize labels with MultiLabelBinarizer\nmlb = MultiLabelBinarizer(classes=most_common_terms)\nY = mlb.fit_transform(train_df[\"labels\"])\n\nprint(\"Label matrix shape:\", Y.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:49:26.713891Z","iopub.execute_input":"2025-11-18T18:49:26.714596Z","iopub.status.idle":"2025-11-18T18:49:28.211049Z","shell.execute_reply.started":"2025-11-18T18:49:26.714565Z","shell.execute_reply":"2025-11-18T18:49:28.210171Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Amino-acid composition features","metadata":{}},{"cell_type":"code","source":"# CHUNK 4: Feature engineering - Amino acid composition\n\nAMINO_ACIDS = list(\"ACDEFGHIKLMNPQRSTVWY\")  # 20 canonical AAs\nAA_INDEX = {aa: i for i, aa in enumerate(AMINO_ACIDS)}\n\ndef featurize_sequence(seq):\n    \"\"\"\n    Simple feature: normalized amino-acid composition + log-length.\n    Returns a 21-dim vector (20 AA freq + log(length)).\n    \"\"\"\n    length = len(seq)\n    counts = np.zeros(len(AMINO_ACIDS), dtype=np.float32)\n\n    for ch in seq:\n        idx = AA_INDEX.get(ch, None)\n        if idx is not None:\n            counts[idx] += 1.0\n\n    if length > 0:\n        counts /= float(length)\n\n    # log-length feature (to not blow up for long proteins)\n    log_len = math.log1p(length)\n\n    return np.concatenate([counts, [log_len]])\n\n\n# Build feature matrix X for all training proteins\nX_list = [featurize_sequence(seq) for seq in train_df[\"sequence\"]]\nX = np.vstack(X_list)\n\nprint(\"Feature matrix shape:\", X.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:49:33.321135Z","iopub.execute_input":"2025-11-18T18:49:33.321974Z","iopub.status.idle":"2025-11-18T18:51:01.229116Z","shell.execute_reply.started":"2025-11-18T18:49:33.321929Z","shell.execute_reply":"2025-11-18T18:51:01.228198Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train/validation split & simple F1 evaluation","metadata":{}},{"cell_type":"code","source":"# CHUNK 5: Train/validation split & model training\n\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X,\n    Y,\n    test_size=0.1,\n    random_state=RANDOM_STATE,\n    stratify=(Y.sum(axis=1) > 0).astype(int)  # stratify by \"has any label\"\n)\n\nprint(\"Train size:\", X_train.shape, \"Valid size:\", X_valid.shape)\n\n# Logistic Regression as base estimator for One-vs-Rest\nbase_lr = LogisticRegression(\n    penalty=\"l2\",\n    solver=\"liblinear\",  # works decently for many small problems\n    max_iter=200\n)\n\nmodel = OneVsRestClassifier(base_lr, n_jobs=-1)\n\nmodel.fit(X_train, y_train)\n\n# Validation predictions & F1\ny_valid_pred_proba = model.predict_proba(X_valid)\n# Use a fixed threshold for now; you can tune it later\nthreshold = 0.5\ny_valid_pred = (y_valid_pred_proba >= threshold).astype(int)\n\nf1_micro = f1_score(y_valid, y_valid_pred, average=\"micro\", zero_division=0)\nf1_macro = f1_score(y_valid, y_valid_pred, average=\"macro\", zero_division=0)\n\nprint(f\"Validation F1 micro: {f1_micro:.4f}\")\nprint(f\"Validation F1 macro: {f1_macro:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:54:12.420458Z","iopub.execute_input":"2025-11-18T18:54:12.420816Z","iopub.status.idle":"2025-11-18T18:55:12.031532Z","shell.execute_reply.started":"2025-11-18T18:54:12.420791Z","shell.execute_reply":"2025-11-18T18:55:12.030787Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Fit on full training data","metadata":{}},{"cell_type":"code","source":"# CHUNK 6: Refit model on all training data\n\n# Free a bit of memory\ndel X_train, X_valid, y_train, y_valid, y_valid_pred, y_valid_pred_proba\ngc.collect()\n\nprint(\"Refitting OneVsRest Logistic Regression on full training data...\")\n\nmodel_full = OneVsRestClassifier(\n    LogisticRegression(\n        penalty=\"l2\",\n        solver=\"liblinear\",\n        max_iter=200\n    ),\n    n_jobs=-1\n)\n\nmodel_full.fit(X, Y)\n\nprint(\"Full model trained.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:55:38.676025Z","iopub.execute_input":"2025-11-18T18:55:38.676910Z","iopub.status.idle":"2025-11-18T18:56:42.852676Z","shell.execute_reply.started":"2025-11-18T18:55:38.676878Z","shell.execute_reply":"2025-11-18T18:56:42.851670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Prepare test sequences & features","metadata":{}},{"cell_type":"code","source":"# CHUNK 7: Load testsuperset sequences and featurize\n\n# correct path according to your directory tree\ntest_fasta_path = os.path.join(DATA_DIR, \"Test\", \"testsuperset.fasta\")\n\nprint(\"Loading test FASTA from:\", test_fasta_path)\n\ntest_sequences = read_fasta(test_fasta_path)\n\nprint(f\"Loaded {len(test_sequences)} test sequences\")\n\n# extract IDs and sequences\ntest_ids = list(test_sequences.keys())\ntest_seqs = [test_sequences[pid] for pid in test_ids]\n\n# featurize\nX_test_list = [featurize_sequence(seq) for seq in test_seqs]\nX_test = np.vstack(X_test_list)\n\nprint(\"Test feature matrix shape:\", X_test.shape)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T18:57:32.325704Z","iopub.execute_input":"2025-11-18T18:57:32.326393Z","iopub.status.idle":"2025-11-18T19:01:13.577413Z","shell.execute_reply.started":"2025-11-18T18:57:32.326361Z","shell.execute_reply":"2025-11-18T19:01:13.576366Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Predict on test & build submission","metadata":{}},{"cell_type":"code","source":"# CHUNK 8: Predict on test and build submission file\n\n# Predict probabilities for all test proteins (multi-label)\ntest_pred_proba = model_full.predict_proba(X_test)\n\nprint(\"Prediction matrix shape:\", test_pred_proba.shape)\n\n# mlb.classes_ gives GO terms in the same order as columns in Y / predictions\ngo_terms_order = mlb.classes_\n\nrows = []\n\nfor i, pid in enumerate(test_ids):\n    probs = test_pred_proba[i]\n\n    # Get indices where probability >= MIN_PRED_PROB\n    idxs = np.where(probs >= MIN_PRED_PROB)[0]\n\n    # Limit up to 1500 terms per protein, as per competition rules\n    if len(idxs) > 1500:\n        # keep top 1500 indices by prob\n        sorted_idxs = np.argsort(probs)[::-1]  # descending\n        idxs = sorted_idxs[:1500]\n\n    for idx in idxs:\n        go_term = go_terms_order[idx]\n        p = probs[idx]\n\n        # Format probability with up to 3 significant figures (not more than 1.000)\n        # Example: 0.931, 0.54, 0.324\n        p_clipped = min(max(p, 1e-6), 1.0)  # enforce (0,1]\n        p_str = f\"{p_clipped:.3g}\"  # 3 significant figures\n\n        rows.append((pid, go_term, p_str))\n\nsubmission_df = pd.DataFrame(rows, columns=[\"target\", \"GO_ID\", \"score\"])\n\nprint(submission_df.head())\nprint(\"Total lines in submission:\", len(submission_df))\n\nsubmission_path = \"submission.tsv\"\nsubmission_df.to_csv(submission_path, sep=\"\\t\", header=False, index=False)\n\nprint(\"Saved submission to\", submission_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:02:50.064812Z","iopub.execute_input":"2025-11-18T19:02:50.065668Z","iopub.status.idle":"2025-11-18T19:03:33.089129Z","shell.execute_reply.started":"2025-11-18T19:02:50.065639Z","shell.execute_reply":"2025-11-18T19:03:33.088215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Quick sanity checks","metadata":{}},{"cell_type":"code","source":"# CHUNK 9: Optional quick checks on submission file\n\n# 1) Any score == 0?\nany_zero = (submission_df[\"score\"].astype(float) == 0.0).any()\nprint(\"Any zero scores?\", any_zero)\n\n# 2) Any protein with >1500 terms?\ncounts_per_prot = submission_df.groupby(\"target\")[\"GO_ID\"].count()\nprint(\"Max number of GO terms per protein in submission:\", counts_per_prot.max())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-18T19:03:35.858619Z","iopub.execute_input":"2025-11-18T19:03:35.858894Z","iopub.status.idle":"2025-11-18T19:03:38.560214Z","shell.execute_reply.started":"2025-11-18T19:03:35.858866Z","shell.execute_reply":"2025-11-18T19:03:38.559444Z"}},"outputs":[],"execution_count":null}]}